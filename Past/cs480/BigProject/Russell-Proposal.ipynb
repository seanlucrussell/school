{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sean Russell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internet radio stations such as Spotify and Pandora are really popular right now. The big selling point of these services is that they can personalize stations, so that each user listens to a custom channel. In order to do this, the services have experts pouring over music looking for particular elements, such as an upbeat tempo or heavy bass, that can be used to connect songs. The thought process goes that if you like one song with heavy bass, you will likely enjoy another one.\n",
    "\n",
    "However, this process is extremely time intensive. Analyzing a 4 minute song can take up to half an hour, and at the rate that content is being generated it is difficult to keep up. In addition, this process is extremely incomplete. Pandora, for instance, looks at 400 different elements to catagorize music. While this is extremely impressive, there are innumerable different ways to classify songs, and only having 400 can lead to oversights.\n",
    "\n",
    "So, the motivation for applying machine learning to music is to reduce the human workload for analyzing music and to increase the accuracy of the catagorization. My goal for this project is to use signal processing and machine learning techniques to being this process, by determining the instrument present in a sample of sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I figure there will be three steps necessary. First, reading in sound from a file. Second, converting that sound into something usable by classification algorithms. And third, classifying the sound using the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in sound files requires only a bit of knowledge about the structure of the file. Audio is stored as a series of amplitudes, as in this image:\n",
    "\n",
    "![image of Samples](http://sce2.umkc.edu/BIT/burrise/it222/notes/sound/sampling.gif)\n",
    "\n",
    "with a little bit of additional header information. So to read in the sound file, just read in the samples. Unfortunately, this data is not particularly easy to work with. Even a second of audio can contain up to 44,000 datapoints, which would make machine learning processes fairly slow. In addition, this data is inflexible, as you would end up with different input sizes depending on how long each sample was. Another problem is that each individual datapoint is essentially useless on its own. The way something sounds is determined by the relative positioning of all of the datapoints. In fact, the way something sounds is determined by the sound waves that compose it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fourier Transform is a way to turn complicated waves into a bunch of simple waves. As in the picture below:\n",
    "\n",
    "![image of Composite Waves](http://dagsaw.sdsu.edu/images/fig3-4.gif)\n",
    "\n",
    "the top three waves are all very simple. However, when they are all added together a much more complicated and interesting wave emerges (the fourth one). This is what fourier analysis is all about. Break a complicated wave up into its simplest components, so that they can then be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image of Fourier Transform](https://i.stack.imgur.com/Y5EAf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, my idea is to convert complicated sound samples into their simplest component parts, the building block waves, and use that as input to the classification algorithms. That way, input sizes can be dramatically reduced and are independent of the length of the sound sample, and also accuratly carry most of the information encoded within a sound. Since there will be a lot of work having to do with turning raw data into usable inputs, I will be relying on algorithms that others have created, both from class and from the internet. I will assemble the data myself, which will be somewhat of an involved process. Fortunately there are a lot of sound samples available for free online, it will just be a matter of collecting the samples and tagging them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Results\n",
    "\n",
    "Hopefully classification of sounds will work based off of only their component waves. If that should work, I would like to try more complicated examples where more than one instrument is playing at a time. I expect that having multiple instruments playing at once will be much more difficult for the algorithm than having to deal with just one, and that classification will not really work without more adjustments to the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "April 14- Implement methods for reading in sound samples and doing fourier analysis\n",
    "\n",
    "April 21- Assemble dataset of sound samples on which to do learning and classification\n",
    "\n",
    "April 28- Use algorithms to classify sound samples, run experiments\n",
    "\n",
    "May 5- Finish writing report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
